# -*- coding: utf-8 -*-
from __future__ import annotations

import os
import re
import json
import time
import math
import shutil
import logging
from dataclasses import dataclass
from pathlib import Path
from datetime import datetime
from collections import defaultdict
from concurrent.futures import ThreadPoolExecutor, as_completed
from typing import Any, Dict, List, Optional, Tuple

import requests
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry
from tqdm import tqdm
from colorama import Fore, Style, init as colorama_init

from ..config.loader import load_merged_config
from ..common.logging_utils import setup_logger

colorama_init(autoreset=True)

PROJECT = "Humanities Doc Toolkit - Sorter"
VERSION = "0.1"


# ----------------------------
# JSON helpers (å…¼å®¹æ¨¡å‹è¿”å› ```json ... ```) [2]
# ----------------------------
def _clean_json_text(text: str) -> str:
    t = (text or "").strip()
    if t.startswith("```"):
        t = re.sub(r"^```[a-zA-Z]*\n", "", t).strip()
        t = t.rstrip("`").strip()
    return t

def _extract_json_obj(text: str) -> Optional[dict]:
    t = _clean_json_text(text)
    if not t:
        return None
    if not (t.startswith("{") and t.endswith("}")):
        m = re.search(r"\{.*\}", t, re.DOTALL)
        if m:
            t = m.group(0)
    try:
        obj = json.loads(t)
        return obj if isinstance(obj, dict) else None
    except Exception:
        return None


# ----------------------------
# API statistics (æ¥è‡ª v5.1 ç»“æ„) [2]
# ----------------------------
class APIStatistics:
    def __init__(self):
        self.stats = {
            "total_calls": 0,
            "successful_calls": 0,
            "failed_calls": 0,
            "tokens_used": 0,
            "api_key_usage": defaultdict(int),
            "service_usage": defaultdict(int),
            "batch_stats": {"first_round_batches": 0, "second_round_calls": 0},
        }

    def record(self, api_key_name: str, service: str, success: bool, tokens: int = 0, call_type: str = "unknown"):
        self.stats["total_calls"] += 1
        self.stats["tokens_used"] += int(tokens or 0)
        self.stats["api_key_usage"][api_key_name] += 1
        self.stats["service_usage"][service] += 1
        if success:
            self.stats["successful_calls"] += 1
        else:
            self.stats["failed_calls"] += 1

        if call_type == "batch_first_round":
            self.stats["batch_stats"]["first_round_batches"] += 1
        elif call_type == "single_content":
            self.stats["batch_stats"]["second_round_calls"] += 1

    def summary(self) -> Dict[str, Any]:
        out = dict(self.stats)
        out["api_key_usage"] = dict(out["api_key_usage"])
        out["service_usage"] = dict(out["service_usage"])
        out["batch_stats"] = dict(out["batch_stats"])
        return out


# ----------------------------
# AI Service selector (v5.1) [2]
# ----------------------------
class AIServiceSelector:
    def __init__(self, merged_cfg: Dict[str, Any]):
        self.cfg = merged_cfg

    def _services(self) -> Dict[str, Any]:
        services = self.cfg.get("ai_services", {}).get("services", {})
        # å…¼å®¹æ—§é…ç½®ä¸­ google=gemini [1]
        if "gemini" not in services and "google" in services:
            services["gemini"] = services["google"]
        # å…¼å®¹æ—§é…ç½®ä¸­ moonshot=kimi [1]
        if "kimi" not in services and "moonshot" in services:
            services["kimi"] = services["moonshot"]
        return services

    def available_services(self) -> List[Dict[str, Any]]:
        out = []
        for name, scfg in self._services().items():
            if not isinstance(scfg, dict) or not scfg.get("enabled", False):
                continue
            keys = [k for k in scfg.get("api_keys", []) if isinstance(k, dict) and k.get("enabled", False) and str(k.get("key", "")).strip()]
            if not keys:
                continue
            out.append({"name": name, "model": scfg.get("model", ""), "api_count": len(keys)})
        return out

    def show_service_menu(self) -> Optional[str]:
        av = self.available_services()
        print(f"{Fore.CYAN}ğŸ¤– è¯·é€‰æ‹©AIæœåŠ¡æä¾›å•†{Style.RESET_ALL}")
        print("=" * 50)

        if not av:
            print(f"{Fore.RED}âŒ æœªæ£€æµ‹åˆ°å¯ç”¨çš„AIæœåŠ¡é…ç½®{Style.RESET_ALL}")
            print("è¯·æ£€æŸ¥ global.yaml ä¸­ ai_services.services.* çš„ enabled ä¸ api_keys")
            return None

        for i, s in enumerate(av, 1):
            print(f"{Fore.WHITE}{i}. {s['name']}{Style.RESET_ALL}")
            print(f"   æ¨¡å‹: {s['model']}")
            print(f"   APIå¯†é’¥æ•°é‡: {s['api_count']} ä¸ª\n")

        while True:
            choice = input(f"{Fore.YELLOW}è¯·é€‰æ‹© (1-{len(av)}): {Style.RESET_ALL}").strip()
            if choice.isdigit():
                idx = int(choice) - 1
                if 0 <= idx < len(av):
                    selected = av[idx]["name"]
                    print(f"{Fore.GREEN}âœ“ å·²é€‰æ‹©: {selected}{Style.RESET_ALL}")
                    return selected
            print(f"{Fore.RED}âŒ æ— æ•ˆé€‰æ‹©,è¯·è¾“å…¥ 1-{len(av)}{Style.RESET_ALL}")


# ----------------------------
# Filtering mode selector (ä»é…ç½®è¯»å–ï¼Œå®Œå…¨å¯¹é½ v5.1 filtering_modes) [1][2]
# ----------------------------
class FilteringModeSelector:
    def __init__(self, tool_cfg: Dict[str, Any]):
        self.cfg = tool_cfg
        self.modes = self._load_modes()

    def _load_modes(self) -> Dict[str, Any]:
        modes = self.cfg.get("document_sorting", {}).get("filtering_modes", {})
        if not isinstance(modes, dict) or not modes:
            raise ValueError("sorter.yaml ç¼ºå°‘ document_sorting.filtering_modes [1]")
        return modes

    def show_mode_menu(self) -> Dict[str, Any]:
        print(f"{Fore.CYAN}ğŸ¯ è¯·é€‰æ‹©ç­›é€‰ç²¾åº¦æ¨¡å¼{Style.RESET_ALL}")
        print("=" * 60)

        keys = list(self.modes.keys())
        for i, k in enumerate(keys, 1):
            m = self.modes[k]
            name = m.get("name", k)
            desc = m.get("description", "")
            fr = m.get("first_round_threshold", 0.3)
            sr = m.get("second_round_threshold", 0.7)
            bs = m.get("batch_size", 8)
            ue = m.get("enable_universal_enhancement", True)
            rs = m.get("run_second_round", True)
            print(f"{i}. {name}")
            if desc:
                print(f"   {desc}")
            print(f"   ç¬¬ä¸€è½®é˜ˆå€¼: {fr}")
            print(f"   ç¬¬äºŒè½®é˜ˆå€¼: {sr}")
            print(f"   æ‰¹æ¬¡å¤§å°: {bs}")
            print(f"   æ™®éåŒ–å¢å¼º: {'å¼€å¯' if ue else 'å…³é—­'}")
            print(f"   ç¬¬äºŒè½®: {'æ‰§è¡Œ' if rs else 'è·³è¿‡(ä»…ç¬¬ä¸€è½®)'}\n")

        default_mode = self.cfg.get("ai", {}).get("default_mode", "balanced")
        default_idx = keys.index(default_mode) + 1 if default_mode in keys else 1

        while True:
            c = input(f"{Fore.YELLOW}è¯·é€‰æ‹© (1-{len(keys)}) [é»˜è®¤{default_idx}]: {Style.RESET_ALL}").strip()
            if not c:
                c = str(default_idx)
            if c.isdigit():
                idx = int(c) - 1
                if 0 <= idx < len(keys):
                    k = keys[idx]
                    m = dict(self.modes[k])
                    m["key"] = k
                    print(f"{Fore.GREEN}âœ“ å·²é€‰æ‹©: {m.get('name', k)}{Style.RESET_ALL}")
                    return m
            print(f"{Fore.RED}âŒ æ— æ•ˆé€‰æ‹©,è¯·è¾“å…¥ 1-{len(keys)}{Style.RESET_ALL}")


# ----------------------------
# Document analyzer (å¯¹é½ v5.1: æ™ºèƒ½ç±»å‹åˆ†æ + å¤šæ ¼å¼æå–) [1][2]
# ----------------------------
class EnhancedDocumentAnalyzer:
    def __init__(self, cfg: Dict[str, Any], logger: logging.Logger):
        self.cfg = cfg
        self.logger = logger
        da = cfg.get("document_analysis", {})
        self.paper_max = int(da.get("page_thresholds", {}).get("paper_max", 50))
        self.book_min = int(da.get("page_thresholds", {}).get("book_min", 100))
        ex = da.get("extraction", {})
        self.paper_pages = int(ex.get("paper_pages", 8))
        self.book_pages = int(ex.get("book_pages", 15))
        self.book_toc_pages = int(ex.get("book_toc_pages", 10))

    def analyze_document_type(self, path: Path) -> Dict[str, Any]:
        page_count = self._get_page_count(path)
        if page_count <= self.paper_max:
            return {"page_count": page_count, "document_type": "paper",
                    "extract_strategy": {"type": "paper", "pages_to_extract": self.paper_pages, "token_budget": 2500, "analyze_toc": False}}
        if page_count >= self.book_min:
            return {"page_count": page_count, "document_type": "book",
                    "extract_strategy": {"type": "book", "pages_to_extract": self.book_pages, "toc_pages": self.book_toc_pages, "token_budget": 3500, "analyze_toc": True}}
        return {"page_count": page_count, "document_type": "medium",
                "extract_strategy": {"type": "medium", "pages_to_extract": min(12, max(1, page_count // 2)), "token_budget": 3000, "analyze_toc": False}}

    def _get_page_count(self, path: Path) -> int:
        suf = path.suffix.lower()
        try:
            if suf == ".pdf":
                import PyPDF2
                with open(path, "rb") as f:
                    r = PyPDF2.PdfReader(f)
                    return len(r.pages)
            if suf in [".docx", ".doc"]:
                from docx import Document
                doc = Document(path)
                para = len([p for p in doc.paragraphs if p.text.strip()])
                return max(1, para // 15)
            # text-like
            txt = path.read_text(encoding="utf-8", errors="ignore")
            return max(1, len(txt) // 3000)
        except Exception:
            return 1

    def extract_smart_content(self, path: Path, strategy: Dict[str, Any]) -> str:
        suf = path.suffix.lower()
        if suf == ".pdf":
            return self._extract_pdf(path, strategy)
        if suf in [".docx", ".doc"]:
            return self._extract_docx(path, strategy)
        if suf in [".txt", ".md", ".rtf"]:
            return self._extract_text(path, strategy)
        return ""

    def _extract_pdf(self, path: Path, strategy: Dict[str, Any]) -> str:
        max_chars = int(strategy.get("token_budget", 2500))
        parts: List[str] = []
        try:
            import pdfplumber
            with pdfplumber.open(path) as pdf:
                total = len(pdf.pages)
                if strategy.get("type") == "book" and strategy.get("analyze_toc"):
                    toc_pages = int(strategy.get("toc_pages", 10))
                    toc = []
                    for i in range(min(toc_pages, total)):
                        t = pdf.pages[i].extract_text() or ""
                        if t.strip():
                            toc.append(t)
                        if sum(len(x) for x in toc) > max_chars // 2:
                            break
                    parts.append("===ç›®å½•ä¸å‰è¨€===\n" + "\n".join(toc)[: max_chars // 2])

                    mid_start = min(toc_pages + 5, max(0, total - 5))
                    sample = []
                    for i in range(mid_start, min(mid_start + 3, total)):
                        t = pdf.pages[i].extract_text() or ""
                        if t.strip():
                            sample.append(t)
                        if sum(len(x) for x in sample) > max_chars // 2:
                            break
                    if sample:
                        parts.append("===å†…å®¹æ ·æœ¬===\n" + "\n".join(sample)[: max_chars // 2])
                else:
                    n = int(strategy.get("pages_to_extract", 8))
                    for i in range(min(n, total)):
                        t = pdf.pages[i].extract_text() or ""
                        if t.strip():
                            parts.append(t)
                        if sum(len(x) for x in parts) > max_chars:
                            break
        except Exception:
            # å›é€€ PyPDF2ï¼ˆä¼šåœ¨æŸäº›PDFä¸Šäº§ç”Ÿ unknown widths å™ªå£°ï¼Œéœ€å±è”½ï¼‰ 
            try:
                import PyPDF2
                import logging as _logging
                from contextlib import redirect_stderr
                from io import StringIO

                # 1) é™ä½ PyPDF2 loggerï¼ˆæœ‰äº›ç‰ˆæœ¬èµ° loggingï¼‰
                _logging.getLogger("PyPDF2").setLevel(_logging.ERROR)

                # 2) å±è”½å†™å…¥ stderr çš„å™ªå£°ï¼ˆunknown widths å¾€å¾€ä»è¿™é‡Œå‡ºæ¥ï¼‰
                _buf = StringIO()
                with redirect_stderr(_buf):
                    with open(path, "rb") as f:
                        r = PyPDF2.PdfReader(f)
                        n = int(strategy.get("pages_to_extract", 8))
                        for i in range(min(n, len(r.pages))):
                            t = r.pages[i].extract_text() or ""
                            if t.strip():
                                parts.append(t)
                            if sum(len(x) for x in parts) > max_chars:
                                break
            except Exception:
                parts = ["æ— æ³•æå–PDFå†…å®¹"]
        return ("\n".join(parts))[:max_chars]

    def _extract_docx(self, path: Path, strategy: Dict[str, Any]) -> str:
        max_chars = int(strategy.get("token_budget", 2000))
        try:
            from docx import Document
            doc = Document(path)
            parts = []
            limit = int(strategy.get("pages_to_extract", 8)) * 15
            for i, p in enumerate(doc.paragraphs):
                if i >= limit:
                    break
                t = (p.text or "").strip()
                if t:
                    parts.append(t)
                if sum(len(x) for x in parts) > max_chars:
                    break
            return ("===DOCX===\n" + "\n".join(parts))[:max_chars]
        except Exception:
            return "æ— æ³•æå–Wordæ–‡æ¡£å†…å®¹"

    def _extract_text(self, path: Path, strategy: Dict[str, Any]) -> str:
        max_chars = int(strategy.get("token_budget", 2000))
        try:
            import chardet
            raw = path.read_bytes()
            enc = (chardet.detect(raw).get("encoding") or "utf-8")
            txt = path.read_text(encoding=enc, errors="ignore")
            return ("===TEXT===\n" + txt)[:max_chars]
        except Exception:
            return "æ— æ³•æå–æ–‡æœ¬å†…å®¹"


# ----------------------------
# API Manager (å®Œæ•´å®ç° v5.1 çš„ä¸‰ä¸ªè°ƒç”¨å…¥å£) [2]
# ----------------------------
class EnhancedAPIManager:
    def __init__(self, merged_cfg: Dict[str, Any], selected_service: str, stats: APIStatistics, logger: logging.Logger):
        self.cfg = merged_cfg
        self.stats = stats
        self.logger = logger
        self.service = selected_service

        services = self.cfg.get("ai_services", {}).get("services", {})
        if "gemini" not in services and "google" in services:
            services["gemini"] = services["google"]
        if "kimi" not in services and "moonshot" in services:
            services["kimi"] = services["moonshot"]

        self.service_cfg = services.get(selected_service)
        if not self.service_cfg:
            raise ValueError(f"æœªæ‰¾åˆ°æœåŠ¡é…ç½®: {selected_service}")

        self.keys = [k for k in self.service_cfg.get("api_keys", []) if k.get("enabled", False) and str(k.get("key", "")).strip()]
        if not self.keys:
            raise ValueError(f"{selected_service} æ²¡æœ‰å¯ç”¨å¯†é’¥")

        self.idx = 0
        self.session = requests.Session()
        retry = Retry(total=2, backoff_factor=0.5, status_forcelist=[429, 500, 502, 503, 504])
        adapter = HTTPAdapter(max_retries=retry, pool_connections=10, pool_maxsize=10)
        self.session.mount("https://", adapter)
        self.session.mount("http://", adapter)

        self.logger.info(f"APIç®¡ç†å™¨å°±ç»ª: {selected_service}, keys={len(self.keys)}")

    def _next_key(self) -> Dict[str, Any]:
        k = self.keys[self.idx % len(self.keys)]
        self.idx += 1
        return k

    def _estimate_tokens(self, prompt: str, answer: str) -> int:
        return len(prompt) // 4 + len(answer) // 4

    def _call(self, prompt: str, key_info: Dict[str, Any], call_type: str) -> str:
        service_name = self.service
        base_url = str(self.service_cfg.get("base_url", "")).rstrip("/")
        model = self.service_cfg.get("model")
        timeout = int(self.service_cfg.get("timeout", 60))
        max_tokens = int(self.service_cfg.get("max_tokens", 3000))
        temperature = float(self.service_cfg.get("temperature", 0.1))

        # Claude [1]
        if service_name == "claude" or "anthropic" in base_url:
            headers = {"x-api-key": key_info["key"], "anthropic-version": "2023-06-01", "content-type": "application/json"}
            payload = {"model": model, "max_tokens": max_tokens, "temperature": temperature, "messages": [{"role": "user", "content": prompt}]}
            t0 = time.time()
            try:
                r = self.session.post(base_url, headers=headers, json=payload, timeout=timeout)
                r.raise_for_status()
                data = r.json()
                text = ""
                if isinstance(data.get("content"), list) and data["content"]:
                    text = data["content"][0].get("text", "")
                tokens = self._estimate_tokens(prompt, text)
                self.stats.record(key_info.get("name", "key"), self.service, True, tokens, call_type=call_type)
                return text
            except Exception as e:
                self.stats.record(key_info.get("name", "key"), self.service, False, 0, call_type=call_type)
                raise e
            finally:
                _ = time.time() - t0

        # Geminiï¼ˆæ—§é…ç½®é‡Œæ˜¯ googleï¼‰[1]
        if service_name == "gemini" or "generativelanguage.googleapis.com" in base_url:
            url = base_url
            if "key=" not in url:
                url = url + ("&" if "?" in url else "?") + f"key={key_info['key']}"
            payload = {"contents": [{"parts": [{"text": prompt}]}],
                       "generationConfig": {"temperature": temperature, "maxOutputTokens": max_tokens}}
            t0 = time.time()
            try:
                r = self.session.post(url, json=payload, timeout=timeout)
                r.raise_for_status()
                data = r.json()
                text = ""
                try:
                    text = data["candidates"][0]["content"]["parts"][0]["text"]
                except Exception:
                    text = ""
                tokens = self._estimate_tokens(prompt, text)
                self.stats.record(key_info.get("name", "key"), self.service, True, tokens, call_type=call_type)
                return text
            except Exception as e:
                self.stats.record(key_info.get("name", "key"), self.service, False, 0, call_type=call_type)
                raise e
            finally:
                _ = time.time() - t0

        # OpenAI-compatibleï¼ˆdeepseek/openai/kimi/glm/qwen ç­‰ï¼‰[1][2]
        headers = {"Content-Type": "application/json", "Authorization": f"Bearer {key_info['key']}"}
        payload = {
            "model": model,
            "messages": [{"role": "user", "content": prompt}],
            "max_tokens": max_tokens,
            "temperature": temperature,
        }
        t0 = time.time()
        try:
            r = self.session.post(base_url, headers=headers, json=payload, timeout=timeout)
            r.raise_for_status()
            data = r.json()
            text = data["choices"][0]["message"]["content"]
            tokens = self._estimate_tokens(prompt, text)
            self.stats.record(key_info.get("name", "key"), self.service, True, tokens, call_type=call_type)
            return text
        except Exception as e:
            self.stats.record(key_info.get("name", "key"), self.service, False, 0, call_type=call_type)
            raise e
        finally:
            _ = time.time() - t0

    # â€”â€” v5.1: æ™®éåŒ–å¢å¼º â€”â€” [2]
    def generate_universal_enhancement_prompt(self, user_query: str) -> str:
        prompt = (
            "ä½ æ˜¯è¯­ä¹‰æ‰©å±•ä¸“å®¶ã€‚è¯·ä¸ºä»¥ä¸‹ç ”ç©¶éœ€æ±‚ç”Ÿæˆæ™®éåŒ–æ¦‚å¿µæ‰©å±•å’Œæœç´¢æç¤ºè¯å¢å¼º:\n\n"
            f"ç”¨æˆ·ç ”ç©¶éœ€æ±‚:{user_query}\n\n"
            "è¯·ç”Ÿæˆ:\n"
            "1. æ ¸å¿ƒæ¦‚å¿µè¯†åˆ«\n"
            "2. æ›´å¹¿æ³›ç›¸å…³æ¦‚å¿µ(å¦‚:åº·å¾·ç§æ—æ€æƒ³ â†’ è¥¿æ–¹ç§æ—è§‚å¿µå²)\n"
            "3. ä¸­è‹±æ–‡ç›¸å…³è¯æ±‡\n"
            "4. å­¦ç§‘äº¤å‰æ¦‚å¿µ\n\n"
            "è¿”å›æ ¼å¼(ä¿æŒç®€æ´):\n"
            "ã€æ™®éåŒ–è¯­ä¹‰æ‰©å±•ã€‘\næ ¸å¿ƒæ¦‚å¿µ:...\nå¹¿æ³›æ¦‚å¿µ:...\nç›¸å…³è¯æ±‡:...\näº¤å‰æ¦‚å¿µ:...\n\n"
            "ã€æœç´¢å¢å¼ºæŒ‡å¯¼ã€‘\næ³¨æ„:ä¸“é—¨ä¸»é¢˜å¯èƒ½ä½œä¸ºæ›´å¹¿æ³›ä¸»é¢˜çš„ç« èŠ‚å‡ºç°,è¯·è¯„ä¼°è¿™ç§åŒ…å«å…³ç³»ã€‚"
        )
        key = self._next_key()
        try:
            text = self._call(prompt, key, call_type="enhancement")
            return f"\n{text}\n"
        except Exception:
            return "\nã€ä½¿ç”¨é»˜è®¤è¯­ä¹‰æ‰©å±•ã€‘\nè¯·è€ƒè™‘ä¸“é—¨ä¸»é¢˜ä¸å¹¿æ³›ä¸»é¢˜çš„åŒ…å«å…³ç³»ã€‚\n"

    # â€”â€” v5.1: ç¬¬ä¸€è½®æ‰¹é‡ â€”â€” [2]
    def batch_first_round_call(self, file_names: List[str], user_query: str, universal_enhancement: str) -> Dict[str, Any]:
        key = self._next_key()
        file_list = "\n".join([f"{i+1}. {name}" for i, name in enumerate(file_names)])
        prompt = f"""ä½ æ˜¯ä¸“ä¸šçš„å­¦æœ¯æ–‡çŒ®è¯„ä¼°ä¸“å®¶ã€‚è¯·åŸºäºæ–‡ä»¶åè¯„ä¼°ä¸ç ”ç©¶éœ€æ±‚çš„ç›¸å…³æ€§ã€‚

ç ”ç©¶éœ€æ±‚:{user_query}

{universal_enhancement}

æ–‡æ¡£åˆ—è¡¨(å…±{len(file_names)}ä¸ª):
{file_list}

è¯„ä¼°è¦æ±‚:
1. è€ƒè™‘ç›´æ¥åŒ¹é…å’Œæ™®éåŒ–å…³ç³»(å¦‚ä¸“é—¨ä¸»é¢˜å¯èƒ½æ˜¯æ›´å¹¿æ³›ä¸»é¢˜çš„ä¸€éƒ¨åˆ†)
2. è¯„ä¼°è¯­ä¹‰ç›¸å…³æ€§,åŒ…æ‹¬è·¨è¯­è¨€ç†è§£
3. è¿”å›æ‰€æœ‰æ–‡æ¡£çš„è¯„åˆ†,æ ¼å¼ä¸¥æ ¼æŒ‰ç…§JSON

è¿”å›æ ¼å¼:
{{"files": [
    {{"name": "å®Œæ•´æ–‡ä»¶å", "score": 0.85, "reason": "è¯¦ç»†åˆ†æç†ç”±"}}
]}}

è¯„åˆ†æ ‡å‡†:
- 0.9-1.0: é«˜åº¦ç›¸å…³
- 0.7-0.8: ç›¸å…³æ€§å¼º
- 0.5-0.6: ä¸­ç­‰ç›¸å…³
- 0.3-0.4: è½»å¾®ç›¸å…³
- 0.0-0.2: ä¸ç›¸å…³

è¯·ç¡®ä¿è¿”å›JSONåŒ…å«æ‰€æœ‰{len(file_names)}ä¸ªæ–‡æ¡£çš„è¯„ä¼°ç»“æœã€‚"""
        try:
            text = self._call(prompt, key, call_type="batch_first_round")
            return {"success": True, "content": text, "api_key": key.get("name", "key"), "batch_size": len(file_names)}
        except Exception as e:
            return {"success": False, "error": str(e), "api_key": key.get("name", "key"), "batch_size": len(file_names)}

    # â€”â€” v5.1: ç¬¬äºŒè½®å•æ–‡æ¡£ â€”â€” [2]
    def single_content_analysis_call(self, document_info: Dict[str, Any], user_query: str, universal_enhancement: str) -> Dict[str, Any]:
        key = self._next_key()
        prompt = f"""è¯·åŸºäºæ–‡æ¡£å®é™…å†…å®¹æ·±åº¦åˆ†æä¸ç ”ç©¶éœ€æ±‚çš„åŒ¹é…åº¦ã€‚

ç ”ç©¶éœ€æ±‚:{user_query}

{universal_enhancement}

æ–‡æ¡£ä¿¡æ¯:
- æ–‡ä»¶å:{document_info['name']}
- æ–‡æ¡£ç±»å‹:{document_info.get('document_type', 'æœªçŸ¥')}
- é¡µæ•°:{document_info.get('page_count', 'æœªçŸ¥')}é¡µ

{document_info.get('content_preview', 'æ— å†…å®¹é¢„è§ˆ')}

åˆ†æè¦æ±‚:
1. åŸºäºå®é™…å†…å®¹(éä»…æ–‡ä»¶å)è¿›è¡Œæ·±åº¦åˆ†æ
2. è€ƒè™‘æ™®éåŒ–å…³ç³»å’Œé—´æ¥ç›¸å…³æ€§
3. è¯„ä¼°å†…å®¹å¯¹ç ”ç©¶éœ€æ±‚çš„å®é™…ä»·å€¼

è¿”å›JSONæ ¼å¼:
{{"score": 0.85, "reason": "åŸºäºå†…å®¹çš„è¯¦ç»†åŒ¹é…åˆ†æ", "content_highlights": "å…³é”®å†…å®¹è¦ç‚¹"}}"""
        try:
            text = self._call(prompt, key, call_type="single_content")
            return {"success": True, "content": text, "api_key": key.get("name", "key")}
        except Exception as e:
            return {"success": False, "error": str(e), "api_key": key.get("name", "key")}


# ----------------------------
# Index scanner (å¯¹é½ v5.1: recursive_scan + supported_formats + skip_hidden + size filter) [1][2]
# ----------------------------
class DocumentIndex:
    def __init__(self, cfg: Dict[str, Any], logger: logging.Logger):
        self.cfg = cfg
        self.logger = logger

    def create_index(self, directory: Path) -> List[Dict[str, Any]]:
        scan = self.cfg.get("document_sorting", {}).get("scanning", {})
        recursive = bool(scan.get("recursive_scan", True))
        supported = set([s.lower() for s in scan.get("supported_formats", [".pdf"])])
        skip_hidden = bool(scan.get("skip_hidden", True))
        min_kb = int(scan.get("min_file_size_kb", 10))
        max_mb = int(scan.get("max_file_size_mb", 500))
        max_depth = int(scan.get("max_depth", 10))

        docs: List[Dict[str, Any]] = []

        def _walk(p: Path, depth: int):
            if depth > max_depth:
                return
            try:
                for item in p.iterdir():
                    if skip_hidden and item.name.startswith("."):
                        continue
                    if item.is_dir():
                        if recursive:
                            _walk(item, depth + 1)
                        continue
                    if item.is_file():
                        suf = item.suffix.lower()
                        if suf not in supported:
                            continue
                        st = item.stat()
                        size_kb = st.st_size / 1024
                        size_mb = st.st_size / 1024 / 1024
                        if size_kb < min_kb:
                            continue
                        if max_mb > 0 and size_mb > max_mb:
                            continue
                        docs.append({
                            "path": str(item),
                            "name": item.name,
                            "stem": item.stem,
                            "suffix": suf,
                            "size": st.st_size,
                            "size_mb": round(size_mb, 2),
                        })
            except Exception:
                return

        _walk(directory, 0)
        docs.sort(key=lambda d: d.get("size", 0))
        return docs


# ----------------------------
# Semantic filter (ä¸¤è½®ç­›é€‰ + fast_first_round) [2][1]
# ----------------------------
class EnhancedSemanticFilter:
    def __init__(self, cfg: Dict[str, Any], api_manager: EnhancedAPIManager, mode: Dict[str, Any], stats: APIStatistics, logger: logging.Logger):
        self.cfg = cfg
        self.api_manager = api_manager
        self.mode = mode
        self.stats = stats
        self.logger = logger

        self.analyzer = EnhancedDocumentAnalyzer(cfg, logger)

        self.first_threshold = float(mode.get("first_round_threshold", 0.3))
        self.second_threshold = float(mode.get("second_round_threshold", 0.7))
        self.batch_size = int(mode.get("batch_size", 8))
        self.enable_universal = bool(mode.get("enable_universal_enhancement", True))
        self.run_second_round = bool(mode.get("run_second_round", True))

        # çº¿ç¨‹é…ç½®æ¥è‡ª sorter.yaml runtime æˆ– v5.1 performance.threading.max_workers [1]
        self.max_threads = int(cfg.get("performance", {}).get("threading", {}).get("max_workers", 4))

        print(f"{Fore.CYAN}ğŸ¯ è¯­ä¹‰ç­›é€‰å™¨å°±ç»ª - {mode.get('name', mode.get('key','mode'))}{Style.RESET_ALL}")
        print(f"   ç¬¬ä¸€è½®é˜ˆå€¼: {self.first_threshold}, æ‰¹æ¬¡: {self.batch_size}/æ‰¹")
        print(f"   ç¬¬äºŒè½®é˜ˆå€¼: {self.second_threshold}, çº¿ç¨‹: {self.max_threads}")
        print(f"   æ™®éåŒ–å¢å¼º: {'å¼€å¯' if self.enable_universal else 'å…³é—­'}")
        print(f"   ç¬¬äºŒè½®: {'æ‰§è¡Œ' if self.run_second_round else 'è·³è¿‡(ä»…ç¬¬ä¸€è½®)'}")

    def first_round_batch_filtering(self, documents: List[Dict[str, Any]], user_query: str) -> List[Dict[str, Any]]:
        print(f"\n{Fore.YELLOW}ğŸ¯ ç¬¬ä¸€è½®æ‰¹é‡æ–‡ä»¶åç­›é€‰({len(documents)} ä¸ªæ–‡æ¡£){Style.RESET_ALL}")
        if not documents:
            return []

        universal = ""
        if self.enable_universal:
            print("   ğŸ§  AIç”Ÿæˆæ™®éåŒ–è¯­ä¹‰å¢å¼º...")
            universal = self.api_manager.generate_universal_enhancement_prompt(user_query)
        else:
            universal = "\nã€è¯­ä¹‰åŒ¹é…æŒ‡å¯¼ã€‘\nè¯·è¿›è¡Œç²¾ç¡®è¯­ä¹‰åŒ¹é…,é¿å…è¿‡åº¦æ™®éåŒ–ã€‚\n"

        batches = [documents[i:i+self.batch_size] for i in range(0, len(documents), self.batch_size)]
        print(f"   ğŸ“¦ æ‰¹æ¬¡é…ç½®:{self.batch_size}ä¸ª/æ‰¹,å…±{len(batches)}æ‰¹")

        passed: List[Dict[str, Any]] = []
        failed_batches = 0

        with ThreadPoolExecutor(max_workers=min(self.max_threads, len(batches))) as ex:
            futs = {}
            for bi, batch in enumerate(batches, 1):
                names = [d["name"] for d in batch]
                futs[ex.submit(self.api_manager.batch_first_round_call, names, user_query, universal)] = (bi, batch)

            with tqdm(total=len(batches), desc=f"ç¬¬ä¸€è½®æ‰¹é‡({self.batch_size}/æ‰¹)", bar_format="{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}]") as bar:
                for fut in as_completed(futs):
                    bi, batch = futs[fut]
                    try:
                        resp = fut.result()
                        if resp.get("success"):
                            obj = _extract_json_obj(resp.get("content", "")) or {}
                            files = obj.get("files", []) if isinstance(obj.get("files", []), list) else []
                            name2doc = {d["name"]: d for d in batch}
                            for item in files:
                                nm = item.get("name")
                                if nm in name2doc:
                                    score = float(item.get("score", 0) or 0)
                                    if score >= self.first_threshold:
                                        d = name2doc[nm]
                                        d["first_round_score"] = score
                                        d["first_round_reason"] = item.get("reason", "")
                                        d["batch_num"] = bi
                                        d["api_key"] = resp.get("api_key", "")
                                        passed.append(d)
                        else:
                            failed_batches += 1
                    except Exception:
                        failed_batches += 1
                    finally:
                        bar.update(1)

        passed.sort(key=lambda x: x.get("first_round_score", 0), reverse=True)
        print(f"{Fore.GREEN}âœ“ ç¬¬ä¸€è½®å®Œæˆ:{len(passed)}/{len(documents)} ä¸ªæ–‡æ¡£é€šè¿‡{Style.RESET_ALL}")
        if failed_batches:
            print(f"{Fore.YELLOW}âš  ç¬¬ä¸€è½®å¤±è´¥æ‰¹æ¬¡: {failed_batches}{Style.RESET_ALL}")
        return passed

    def second_round_content_filtering(self, documents: List[Dict[str, Any]], user_query: str) -> List[Dict[str, Any]]:
        print(f"\n{Fore.YELLOW}ğŸ”¬ ç¬¬äºŒè½®å†…å®¹åˆ†æ({len(documents)}ä¸ªæ–‡æ¡£){Style.RESET_ALL}")
        if not documents:
            return []

        universal = ""
        if self.enable_universal:
            universal = self.api_manager.generate_universal_enhancement_prompt(user_query)
        else:
            universal = "\nã€ç²¾ç¡®å†…å®¹åˆ†æã€‘\nè¯·è¿›è¡Œç²¾ç¡®å†…å®¹åŒ¹é…,é¿å…è¿‡åº¦æ™®éåŒ–ã€‚\n"

        # é¢„å¤„ç†ï¼šç±»å‹åˆ†æ + å†…å®¹æå–ï¼ˆå¯¹åº” v5.1 çš„é¢„å¤„ç†è¿›åº¦æ¡ï¼‰[2]
        processed: List[Dict[str, Any]] = []
        for d in tqdm(documents, desc="é¢„å¤„ç†", leave=True, bar_format="{l_bar}{bar}| {n_fmt}/{total_fmt}"):
            analysis = self.analyzer.analyze_document_type(Path(d["path"]))
            d.update(analysis)
            content = self.analyzer.extract_smart_content(Path(d["path"]), analysis["extract_strategy"])
            d["content_preview"] = content
            processed.append(d)

        passed: List[Dict[str, Any]] = []
        failed = 0

        with ThreadPoolExecutor(max_workers=self.max_threads) as ex:
            futs = {ex.submit(self.api_manager.single_content_analysis_call, d, user_query, universal): d for d in processed}
            with tqdm(total=len(processed), desc="AIåˆ†æ", bar_format="{l_bar}{bar}| {n_fmt}/{total_fmt}") as bar:
                for fut in as_completed(futs):
                    d = futs[fut]
                    try:
                        resp = fut.result()
                        if resp.get("success"):
                            obj = _extract_json_obj(resp.get("content", "")) or {}
                            score = float(obj.get("score", 0) or 0)
                            if score >= self.second_threshold:
                                d["second_round_score"] = score
                                d["second_round_reason"] = obj.get("reason", "")
                                d["content_highlights"] = obj.get("content_highlights", "")
                                d["api_key_used"] = resp.get("api_key", "")
                                passed.append(d)
                        else:
                            failed += 1
                    except Exception:
                        failed += 1
                    finally:
                        bar.update(1)

        passed.sort(key=lambda x: x.get("second_round_score", 0), reverse=True)
        if failed:
            print(f"{Fore.YELLOW}âš  ç¬¬äºŒè½®åˆ†æå¤±è´¥: {failed}ä¸ªæ–‡æ¡£{Style.RESET_ALL}")
        print(f"{Fore.GREEN}âœ“ ç¬¬äºŒè½®å®Œæˆ: {len(passed)}/{len(documents)}ä¸ªæ–‡æ¡£é€šè¿‡{Style.RESET_ALL}")
        return passed


# ----------------------------
# Folder naming (å¯¹é½ v5.1 folder_naming) [1]
# ----------------------------
class FolderNamer:
    def __init__(self, cfg: Dict[str, Any]):
        fn = cfg.get("folder_naming", {})
        self.auto_generate = bool(fn.get("auto_generate", True))
        self.max_name_length = int(fn.get("max_name_length", 30))
        self.add_timestamp = bool(fn.get("add_timestamp", True))
        self.timestamp_format = fn.get("timestamp_format", "%m%d_%H%M")
        self.forbidden = set(fn.get("forbidden_chars", ['<', '>', ':', '"', '/', '\\', '|', '?', '*']))

    def make(self, user_query: str) -> str:
        base = user_query.strip()
        if not base:
            base = "AIæ™ºèƒ½åˆ†æ‹£"
        # ç®€å•æŠ½å–å‰3ä¸ªè¯ï¼ˆä¸ v5.1 generate_folder_name ç›¸ä¼¼ï¼‰[2]
        base = base.replace("çš„", " ").replace("å’Œ", " ")
        words = [w for w in re.split(r"\s+", base) if w]
        base_name = "_".join(words[:3]) if words else "AIæ™ºèƒ½åˆ†æ‹£"
        base_name = "".join(ch for ch in base_name if ch not in self.forbidden)
        base_name = base_name[: self.max_name_length]

        if self.add_timestamp:
            ts = datetime.now().strftime(self.timestamp_format)
            base_name = f"{base_name}_{ts}"
        return base_name[: self.max_name_length]


# ----------------------------
# Report generator (å¯¹é½ v5.1: æ™ºèƒ½åˆ†æ‹£è¯¦ç»†æŠ¥å‘Š.txt + APIç»Ÿè®¡ + ä¸¤è½®è¯„åˆ†ç†ç”±) [2]
# ----------------------------
class ReportWriter:
    def __init__(self, cfg: Dict[str, Any], stats: APIStatistics, logger: logging.Logger):
        self.cfg = cfg
        self.stats = stats
        self.logger = logger

    def write(self, documents: List[Dict[str, Any]], target_folder: Path, user_query: str, mode: Dict[str, Any]):
        log_cfg = self.cfg.get("logging", {})
        if not bool(log_cfg.get("generate_detailed_report", True)):
            return

        report_name = log_cfg.get("report_filename", "æ™ºèƒ½åˆ†æ‹£è¯¦ç»†æŠ¥å‘Š.txt")
        report_path = target_folder / report_name
        s = self.stats.summary()

        try:
            with report_path.open("w", encoding="utf-8") as f:
                f.write(f"Smart Document Sorter v5.1 - æ ¸å¿ƒé€»è¾‘ç§»æ¤ç‰ˆï¼ˆå·¥å…·é“¾ï¼‰\n")
                f.write("=" * 60 + "\n\n")
                f.write(f"ç”¨æˆ·ç ”ç©¶éœ€æ±‚: {user_query}\n")
                f.write(f"ç­›é€‰æ¨¡å¼: {mode.get('name', mode.get('key','mode'))}\n")
                f.write(f"åˆ†æ‹£æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
                f.write(f"ç»“æœæ–‡ä»¶å¤¹: {target_folder}\n")
                f.write(f"æœ€ç»ˆæ–‡æ¡£æ•°é‡: {len(documents)}\n")
                f.write(f"åˆ†ææ–¹æ³•: ä¸¤è½®ç­›é€‰ï¼ˆæ–‡ä»¶åæ‰¹é‡ + å†…å®¹æ·±åº¦ï¼‰+ å¯é€‰æ™®éåŒ–å¢å¼º\n\n")

                f.write("APIè°ƒç”¨ç»Ÿè®¡:\n")
                f.write("-" * 30 + "\n")
                f.write(f"æ€»è°ƒç”¨æ¬¡æ•°: {s['total_calls']}\n")
                f.write(f"æˆåŠŸè°ƒç”¨: {s['successful_calls']}\n")
                f.write(f"å¤±è´¥è°ƒç”¨: {s['failed_calls']}\n")
                f.write(f"Tokenä½¿ç”¨é‡(ä¼°ç®—): {s['tokens_used']:,}\n")
                f.write(f"ç¬¬ä¸€è½®æ‰¹æ¬¡: {s['batch_stats']['first_round_batches']}\n")
                f.write(f"ç¬¬äºŒè½®å•åˆ†æ: {s['batch_stats']['second_round_calls']}\n\n")

                f.write("APIå¯†é’¥ä½¿ç”¨åˆ†å¸ƒ:\n")
                for k, c in sorted(s["api_key_usage"].items(), key=lambda x: x[1], reverse=True):
                    f.write(f" - {k}: {c}\n")
                f.write("\n")

                f.write("æ–‡æ¡£åˆ†æç»“æœæ˜ç»†:\n")
                f.write("-" * 60 + "\n\n")
                for i, doc in enumerate(documents, 1):
                    f.write(f"ã€{i:3d}ã€‘ {doc['name']}\n")
                    f.write(f"     æ–‡ä»¶å¤§å°: {doc.get('size_mb', 0):.2f} MB\n")
                    f.write(f"     æ–‡æ¡£ç±»å‹: {doc.get('document_type', 'æœªçŸ¥')}\n")
                    f.write(f"     é¡µæ•°ä¼°ç®—: {doc.get('page_count', 'æœªçŸ¥')} é¡µ\n")

                    if "first_round_score" in doc:
                        f.write(f"     ç¬¬ä¸€è½®è¯„åˆ†: {doc.get('first_round_score', 0):.2f}\n")
                        f.write(f"     æ–‡ä»¶ååˆ†æ: {str(doc.get('first_round_reason',''))[:120]}...\n")

                    if "second_round_score" in doc:
                        f.write(f"     ç¬¬äºŒè½®è¯„åˆ†: {doc.get('second_round_score', 0):.2f}\n")
                        f.write(f"     å†…å®¹åˆ†æ: {str(doc.get('second_round_reason',''))[:160]}...\n")
                        if doc.get("content_highlights"):
                            f.write(f"     å†…å®¹è¦ç‚¹: {str(doc.get('content_highlights',''))[:160]}...\n")

                    f.write(f"     åŸå§‹è·¯å¾„: {doc['path']}\n")
                    f.write("\n" + "-" * 50 + "\n\n")

            print(f"{Fore.GREEN}âœ“ è¯¦ç»†æŠ¥å‘Šå·²ç”Ÿæˆ: {report_path}{Style.RESET_ALL}")
        except Exception as e:
            self.logger.warning(f"ç”ŸæˆæŠ¥å‘Šå¤±è´¥: {e}")


# ----------------------------
# Sorter main (äº¤äº’ + ä¸¤è½® + copy) [2][1]
# ----------------------------
class SorterEngine:
    def __init__(self, global_config: str, tool_config: str):
        self.cfg = load_merged_config(global_config, tool_config)
        self.logger = setup_logger("hdt-sorter", str(self.cfg.get("logging", {}).get("level", "INFO")))
        self.stats = APIStatistics()

    def _paths(self) -> Tuple[str, str]:
        # è¾“å…¥/è¾“å‡ºï¼šéµå¾ª v5.1 â€œè¿è¡Œæ—¶è¾“å…¥ inputï¼Œè¾“å‡ºé»˜è®¤ Desktopâ€é£æ ¼ [1][2]
        input_dir = input(f"{Fore.CYAN}ğŸ“‚ æºæ–‡ä»¶å¤¹è·¯å¾„: {Style.RESET_ALL}").strip().strip('"')
        if not input_dir:
            raise ValueError("æºæ–‡ä»¶å¤¹è·¯å¾„ä¸èƒ½ä¸ºç©º")
        if not Path(input_dir).expanduser().exists():
            raise ValueError("æºæ–‡ä»¶å¤¹è·¯å¾„ä¸å­˜åœ¨")

        default_output = str((Path.cwd() / "output").resolve())
        out_dir = input(f"{Fore.CYAN}ğŸ“ è¾“å‡ºç›®å½• (é»˜è®¤: {default_output}): {Style.RESET_ALL}").strip().strip('"') or default_output
        Path(out_dir).expanduser().mkdir(parents=True, exist_ok=True)
        return str(Path(input_dir).expanduser()), str(Path(out_dir).expanduser())

    def _user_query(self) -> str:
        while True:
            q = input(f"{Fore.CYAN}ğŸ” ç ”ç©¶éœ€æ±‚: {Style.RESET_ALL}").strip()
            if q:
                return q
            print(f"{Fore.RED}âŒ éœ€æ±‚ä¸èƒ½ä¸ºç©º{Style.RESET_ALL}")

    def _thread_config(self) -> int:
        # v5.1: ai_services.threading_config.max_threads (è¿è¡Œæ—¶å¯è‡ªå®šä¹‰) [1][2]
        th_cfg = self.cfg.get("ai_services", {}).get("threading_config", {})
        default_threads = int(th_cfg.get("max_threads", self.cfg.get("runtime", {}).get("max_threads_default", 4)))

        while True:
            s = input(f"{Fore.CYAN}âš™ï¸  æœ€å¤§çº¿ç¨‹æ•° (é»˜è®¤ {default_threads}): {Style.RESET_ALL}").strip()
            if not s:
                return default_threads
            if s.isdigit():
                v = int(s)
                if 1 <= v <= 32:
                    return v
            print(f"{Fore.RED}âŒ çº¿ç¨‹æ•°å¿…é¡»åœ¨1-32ä¹‹é—´{Style.RESET_ALL}")

    def _apply_thread_policy(self, selected_service: str, requested_threads: int) -> int:
        # ä¾æ® v5.1 api_pool_config.buffer_ratio çš„å†—ä½™æ€æƒ³ [1]
        buffer_ratio = float(self.cfg.get("ai_services", {}).get("api_pool_config", {}).get("buffer_ratio", 1.5))
        services = self.cfg.get("ai_services", {}).get("services", {})
        if "gemini" not in services and "google" in services:
            services["gemini"] = services["google"]
        if "kimi" not in services and "moonshot" in services:
            services["kimi"] = services["moonshot"]

        keys = [k for k in services.get(selected_service, {}).get("api_keys", []) if k.get("enabled", False) and str(k.get("key","")).strip()]
        available_keys = len(keys)
        if available_keys <= 0:
            return 1

        required = math.ceil(requested_threads * buffer_ratio)
        if available_keys < required:
            # è‡ªåŠ¨é™çº§çº¿ç¨‹ï¼šfloor(keys / ratio)
            safe_threads = max(1, int(available_keys // buffer_ratio))
            print(f"{Fore.YELLOW}âš  å¯†é’¥æ•°é‡ä¸è¶³ä»¥æ»¡è¶³ {buffer_ratio}x å†—ä½™ï¼šthreads={requested_threads} éœ€è¦keysâ‰ˆ{required}ï¼Œå½“å‰keys={available_keys}ã€‚å°†è‡ªåŠ¨é™ä¸º {safe_threads} çº¿ç¨‹ã€‚{Style.RESET_ALL}")
            return safe_threads

        return requested_threads

    def _select_service(self) -> str:
        show = bool(self.cfg.get("ai", {}).get("show_service_selection", True))
        selector = AIServiceSelector(self.cfg)
        if show:
            svc = selector.show_service_menu()
            if not svc:
                raise ValueError("æœªé€‰æ‹©AIæœåŠ¡")
            return svc
        # é»˜è®¤ active_service [1]
        svc = self.cfg.get("ai_services", {}).get("active_service", "deepseek")
        return svc

    def _select_mode(self) -> Dict[str, Any]:
        show = bool(self.cfg.get("ai", {}).get("show_mode_selection", True))
        selector = FilteringModeSelector(self.cfg)
        if show:
            return selector.show_mode_menu()
        default_mode = self.cfg.get("ai", {}).get("default_mode", "balanced")
        modes = self.cfg.get("document_sorting", {}).get("filtering_modes", {})
        m = dict(modes.get(default_mode, list(modes.values())[0]))
        m["key"] = default_mode
        return m

    def _copy_documents(self, docs: List[Dict[str, Any]], target: Path) -> Dict[str, Any]:
        target.mkdir(parents=True, exist_ok=True)
        ok, fail = [], []

        with tqdm(total=len(docs), desc="å¤åˆ¶æ–‡ä»¶", bar_format="{l_bar}{bar}| {n_fmt}/{total_fmt}") as bar:
            for d in docs:
                try:
                    src = Path(d["path"])
                    dst = target / src.name
                    if dst.exists():
                        stem, suf = dst.stem, dst.suffix
                        c = 1
                        while dst.exists():
                            dst = target / f"{stem}_{c}{suf}"
                            c += 1
                    shutil.copy2(src, dst)
                    ok.append({"source": str(src), "target": str(dst), "doc": d})
                except Exception as e:
                    fail.append({"path": d.get("path"), "error": str(e)})
                finally:
                    bar.update(1)

        return {"success": ok, "failed": fail}

    def run_interactive(self) -> int:
        from ..__about__ import __title__, __version__, __author__, __email__, __github__

        print(f"{Fore.MAGENTA}{'='*60}{Style.RESET_ALL}")
        print(f"{Fore.MAGENTA}ğŸ§  {__title__} - Sorter v{__version__}{Style.RESET_ALL}")
        print(f"{Fore.CYAN}ğŸ‘¨â€ğŸ’» ä½œè€…: {__author__} | {__email__}{Style.RESET_ALL}")
        print(f"{Fore.CYAN}ğŸ”— {__github__}{Style.RESET_ALL}")
        print(f"{Fore.WHITE}æç¤º:{Style.RESET_ALL}")
        print(f"{Fore.WHITE}- é»˜è®¤æ“ä½œæ˜¯ copyï¼ˆä¿ç•™åŸæ–‡ä»¶ï¼Œé™ä½è¯¯æ“ä½œé£é™©ï¼‰[6]{Style.RESET_ALL}")
        print(f"{Fore.WHITE}- ä¸¤è½®ç­›é€‰ï¼šæ–‡ä»¶åæ‰¹é‡ + å†…å®¹æ·±åº¦ï¼ˆå¯é€‰ fast_first_round ä»…ç¬¬ä¸€è½®ï¼‰[6]{Style.RESET_ALL}")
        print(f"{Fore.MAGENTA}{'='*60}{Style.RESET_ALL}")
        import logging
        logging.getLogger("PyPDF2").setLevel(logging.ERROR)
        selected_service = self._select_service()
        mode = self._select_mode()

        source_dir, output_dir = self._paths()
        user_query = self._user_query()

        requested_threads = self._thread_config()
        max_threads = self._apply_thread_policy(selected_service, requested_threads)

        # å†™å…¥ runtime æ€§èƒ½é…ç½®ï¼ˆä¸ v5.1 â€œè¿è¡Œæ—¶å¯è‡ªå®šä¹‰çº¿ç¨‹â€ä¸€è‡´ï¼‰[1][2]
        self.cfg.setdefault("performance", {}).setdefault("threading", {})["max_workers"] = max_threads

        print(f"\n{Fore.CYAN}ğŸ“‹ åˆ†æ‹£ä»»åŠ¡é…ç½®:{Style.RESET_ALL}")
        print(f"   æºç›®å½•: {Path(source_dir).name}")
        print(f"   éœ€æ±‚: {user_query}")
        print(f"   ç­›é€‰æ¨¡å¼: {mode.get('name', mode.get('key','mode'))}")
        print(f"   æœ€å¤§çº¿ç¨‹: {max_threads}")
        print(f"   è¾“å‡º: {Path(output_dir).name}")
        print(f"   æ“ä½œ: copy\n")

        start = time.time()

        indexer = DocumentIndex(self.cfg, self.logger)
        docs = indexer.create_index(Path(source_dir))
        if not docs:
            print(f"{Fore.RED}âŒ æœªæ‰¾åˆ°æ”¯æŒçš„æ–‡æ¡£{Style.RESET_ALL}")
            return 1

        api_manager = EnhancedAPIManager(self.cfg, selected_service, self.stats, self.logger)
        semantic_filter = EnhancedSemanticFilter(self.cfg, api_manager, mode, self.stats, self.logger)

        # ç¬¬ä¸€è½®
        first_pass = semantic_filter.first_round_batch_filtering(docs, user_query)
        if not first_pass:
            print(f"{Fore.RED}âŒ ç¬¬ä¸€è½®ç­›é€‰æ— ç»“æœ{Style.RESET_ALL}")
            return 1

        # ç¬¬äºŒè½®ï¼ˆå¯é€‰ï¼šfast_first_round è·³è¿‡ï¼‰[1]
        if semantic_filter.run_second_round:
            second_pass = semantic_filter.second_round_content_filtering(first_pass, user_query)
            final_docs = second_pass if second_pass else first_pass[:15]
            if not second_pass:
                print(f"{Fore.YELLOW}âš  ç¬¬äºŒè½®æ— ç»“æœï¼Œä½¿ç”¨ç¬¬ä¸€è½®Topç»“æœ{Style.RESET_ALL}")
        else:
            final_docs = first_pass[: min(200, len(first_pass))]
            print(f"{Fore.YELLOW}âš¡ å·²é€‰æ‹©ä»…ç¬¬ä¸€è½®æ¨¡å¼ï¼šå°†ç›´æ¥ä½¿ç”¨ç¬¬ä¸€è½®ç»“æœï¼ˆTop {len(final_docs)}ï¼‰{Style.RESET_ALL}")

        # è¾“å‡ºæ–‡ä»¶å¤¹
        folder_name = FolderNamer(self.cfg).make(user_query)
        target_folder = Path(output_dir) / folder_name

        results = self._copy_documents(final_docs, target_folder)

        # æŠ¥å‘Š
        ReportWriter(self.cfg, self.stats, self.logger).write(final_docs, target_folder, user_query, mode)

        elapsed = time.time() - start
        self._show_summary(results, target_folder, elapsed, mode)
        return 0

    def _show_summary(self, results: Dict[str, Any], target_folder: Path, elapsed: float, mode: Dict[str, Any]):
        ok = len(results.get("success", []))
        fail = len(results.get("failed", []))
        s = self.stats.summary()

        print(f"\n{Fore.MAGENTA}ğŸ‰ æ™ºèƒ½åˆ†æ‹£å®Œæˆ!{Style.RESET_ALL}")
        print("=" * 60)
        print(f"{Fore.CYAN}ğŸ“Š ç»“æœç»Ÿè®¡:{Style.RESET_ALL}")
        print(f"   æˆåŠŸåˆ†æ‹£: {ok} ä¸ªæ–‡æ¡£")
        print(f"   å¤±è´¥: {fail} ä¸ªæ–‡æ¡£")
        print(f"   å¤„ç†ç”¨æ—¶: {elapsed:.1f} ç§’")
        if elapsed > 0:
            print(f"   å¹³å‡é€Ÿåº¦: {ok/elapsed:.2f} æ–‡æ¡£/ç§’")
        print(f"   ç­›é€‰æ¨¡å¼: {mode.get('name', mode.get('key','mode'))}")

        print(f"\n{Fore.YELLOW}ğŸ¤– APIè°ƒç”¨ç»Ÿè®¡:{Style.RESET_ALL}")
        if s["total_calls"] > 0:
            print(f"   æ€»è°ƒç”¨æ¬¡æ•°: {s['total_calls']}")
            print(f"   æˆåŠŸç‡: {s['successful_calls']/s['total_calls']*100:.1f}%")
        print(f"   Tokenä½¿ç”¨(ä¼°ç®—): {s['tokens_used']:,}")
        print(f"   ç¬¬ä¸€è½®æ‰¹æ¬¡: {s['batch_stats']['first_round_batches']}")
        print(f"   ç¬¬äºŒè½®å•åˆ†æ: {s['batch_stats']['second_round_calls']}")

        if s["api_key_usage"]:
            print(f"\n{Fore.GREEN}ğŸ”‘ APIå¯†é’¥ä½¿ç”¨åˆ†å¸ƒ:{Style.RESET_ALL}")
            for k, c in sorted(s["api_key_usage"].items(), key=lambda x: x[1], reverse=True):
                print(f"   {k}: {c} æ¬¡")

        print(f"\n{Fore.BLUE}ğŸ“ ä¿å­˜ä½ç½®: {target_folder}{Style.RESET_ALL}")
        print(f"{Fore.BLUE}ğŸ“„ è¯¦ç»†æŠ¥å‘Š: {target_folder}/æ™ºèƒ½åˆ†æ‹£è¯¦ç»†æŠ¥å‘Š.txt{Style.RESET_ALL}")

        try:
            if input(f"\n{Fore.CYAN}æ‰“å¼€ç»“æœæ–‡ä»¶å¤¹? (y/N): {Style.RESET_ALL}").lower() == "y":
                import subprocess, platform
                if platform.system() == "Windows":
                    subprocess.run(f'explorer "{target_folder}"', shell=True)
                elif platform.system() == "Darwin":
                    subprocess.run(["open", str(target_folder)])
                else:
                    subprocess.run(["xdg-open", str(target_folder)])
        except Exception:
            pass